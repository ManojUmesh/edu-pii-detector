# Automated Detection and Anonymization of Personally Identifiable Information (PII) in Educational Datasets

This project implements a hybrid machine learning and natural language processing (NLP) system for detecting and anonymizing **Personally Identifiable Information (PII)** in educational datasets. It was developed as part of the MSc in Computing at Atlantic Technological University (ATU).

The system combines:
- **Rule-based regex patterns** for structured entities (emails, phone numbers, URLs).
- **Classical machine learning baselines** (Logistic Regression, SVM, SGD) using TFâ€“IDF features.
- **Transformer-based models** (BERT and DeBERTa-v3-small) fine-tuned for token-level PII detection.
- **Anonymization module** that replaces detected entities with placeholders (e.g., `<NAME>`, `<EMAIL>`).

---

## Features

- Preprocessing pipeline for structured (CSV/JSON) and unstructured text (`TXT/DOCX/PDF/CSV`) inputs.
- Regex-based entity detection for deterministic PII types.
- Baseline ML models with TFâ€“IDF features.
- Fine-tuned transformer models (BERT, DeBERTa) for contextual detection.
- Token-level evaluation metrics: precision, recall, F1-score.
- Confusion Matrix, ROC curves, and Precisionâ€“Recall curves.
- Deterministic anonymization (tag replacement).
- Sample input/output examples (see Appendices in thesis).

---

## Requirements

- Python **3.11+**
- Jupyter Notebook / Google Colab (for experiments)
- [HuggingFace Transformers](https://huggingface.co/transformers/)
- TensorFlow / Keras
- Scikit-learn
- SpaCy (with `en_core_web_sm` model)
- pandas, numpy, matplotlib, seaborn
- pdfplumber, python-docx (for document parsing)

Install all dependencies with:

```bash
pip install -r requirements.txt
```

---

## ðŸš€ Setup & Compilation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/ManojUmesh/edu-pii-detector.git
   cd edu-pii-detector
   ```

2. **Download dataset**:  
   - Primary dataset: [Kaggle â€“ PII Detection & Removal from Educational Data](https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/data)  
   - Place the JSON/CSV files inside the `input/` folder.

3. **Train a model**:
   Update ModelConfiguration.train=True in config.py
   ```bash
   python src/main.py
   ```

4. **Run predictions**:
   If ModelConfiguration.train=False and no --input file is provided:
   ```bash
   python src/main.py
   ```
   This will run predictions on the test dataset JSON defined in config.py

   Outputs:
   1.submission.csv â†’ entity predictions.
   2.processed_data.csv â†’ anonymized version of test data.

5. **Run predictions on a customfile**:
   ```bash
   python src/main.py --input sample.txt
   ```
   Supported formats: .txt, .csv, .docx, .pdf.

---

## ðŸ“Š Evaluation

You can reproduce the evaluation results using the provided Jupyter notebooks:

- `notebooks/classical_ml_baseline.ipynb` â€“ Implements baseline models such as Logistic Regression, SVM, and SGD.  
- `notebooks/pii_evaluation.ipynb` â€“ Evaluates transformer-based models and generates confusion matrices, ROC curves, and Precisionâ€“Recall curves.

The following metrics are reported:

- **Precision, Recall, and F1-score** (both token-level and entity-level).  
- **Confusion Matrix** (absolute counts and normalized percentages).  
- **Receiver Operating Characteristic (ROC) curves**.  
- **Precisionâ€“Recall (PR) curves**.


---

## ðŸ“‚ Repository Structure

```
project/
â”œâ”€â”€ input/                     # Dataset files
â”œâ”€â”€ models/                   # Saved model weights
â”œâ”€â”€ src/                  # Core Python scripts
â”‚   â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ io_ingest.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ predict.py
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ processed_data.csv
â”‚   â”œâ”€â”€ sample.txt
â”‚   â”œâ”€â”€ submission.csv
â”‚   â””â”€â”€ training.py
â”œâ”€â”€ notebooks/                # Jupyter/Colab notebooks
â”‚   â”œâ”€â”€ classical_ml_baseline.ipynb
â”‚   â””â”€â”€ pii_evaluation.ipynb
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## Notes & Limitations

- This project was developed using **public Kaggle datasets** only. No real student data was collected.  
- Current support is limited to English text.  
- Transformer models require GPU resources (Google Colab recommended).  
- GUI and advanced anonymization metrics (e.g., Utility Preservation Index) are identified as **future work**.

---

## Acknowledgments

This project was developed as part of the MSc in Computing at **Atlantic Technological University (ATU), Galway**.  

Generative AI tools (OpenAIâ€™s ChatGPT) were used for scaffolding portions of preprocessing and evaluation code; all code was reviewed, adapted, and validated by the author.
